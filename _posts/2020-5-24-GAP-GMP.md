---
title: Global Average Pooling
tags: [Deep Learning, Artificial Intelligence, Pooling]
style: fill
color: secondary
description: My understanding of this layer.
---

## Global Average (Max) Pooling 
        
## Motivation:
Global average (max) pooling is used to minimize the overfitting by reducing the number of params in the model. Global average (max) pooling is simillar to normal
average (max) pooling which is used to reduce the spatial dimensions of a three dimensional tensor. However, Global average (max) pooling tends to perform type of
dimensionality reduction where a tensor with dimensions of <i>h x w x d</i> is reduced in size to have dimensions of <i>1 x 1 x d</i> by simply taking the average (max)
value of the channel.

## Algorithm:
The illustration of Global Average (Max) Pooling is shown in the image below:
![image info](assets/img/GAP_GMP.png)

Let's say the number of feature maps in the last conv layer is 3 and each size is 3x3.

In the example above, GAP adds all values ​​and divide by the number of value in each feature map, and GMP creates a vector by picking the maximum value among all values. 
(In fact, if you take the average, you should divide 1+2+1 and divide it by 9, but it is treated as a sum in the paper. 
Anyway, I understood that all feature maps of the same layer have the same number of x and y, so it is not necessary to divide them.)



## Benefits

Replacing the Global Average (Max) Pooling helps removing a large number of trainable parameters from the model.
A <i>7 x 7 x 64</i> CNN output being flattened and fed into a 500 node dense layer would come out with 1.56 million weights. 
Removing them helps training faster and avoiding overfitting which caused by too heavy network.

People argue that removing the fully conneected layers forces the feature maps to be more closely related to the classification categories.

Due to the averaging operation over the feature maps, this makes the model more robust to spatial translations in the data. 
In other words, as long as the requisite feature is included / or activated in the feature map somewhere, 
it will still be “picked up” by the averaging operation. <a href="https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/#:~:text=Global%20Average%20Pooling%20has%20the,layers%20have%20lots%20of%20parameters.">[ref]</a></a>
        